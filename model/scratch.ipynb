{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** generating the forcing file, please be patient **********\n",
      "********** should take ~3 minutes to run **********\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # SNOTEL Processing for pySUMMA Modeling\n",
    "# \n",
    "# This notebook pulls SNOTEL data via metloom to create a meteorological forcing file for pysumma. 7 input meteorological variables are needed at hourly (if using SNOTEL data, hourly is highest temporal resolution possible) timesteps: air temperature, precipitation, incoming shortwave radiation, incoming longwave radiation, air pressure, relative humidity, and wind speed. \n",
    "# \n",
    "# Temperature used is observed from SNOTEL with the *Currier et al. (2017)* voltage issue correction. Precipitation used is observed from SNOTEL - be wary of undercatch for upper elevation SNOTEL sites in windier locations, check to ensure accumulated precip > max SWE. Incoming shortwave radiation is empirically derived using latitude, elevation, and time of year to calculate clear sky radiation and the diurnal temperature range and precipitation to calculate the cloud correction factor with the MetSim package. Incoming longwave radiation is empirically derived from air temperature and relative humidity using the *Dilley and O'Brien (1998)* method. The empirical calculation method for incoming longwave radiation can be modified if desired - the `lw_clr.py` script in `summa_work/utils` provides a number of different methods to choose from. Relative humidity is empirically derived assuming the running 24 hour minimum temperature as the dewpoint for each timestep from *Running et al. (2017)*. Wind speed is set at 2 meters per second for every timestep as this is an incredibly difficult quantity to observe in mountainous regions during winter due to riming and other issues (*TODO - citation needed for this choice*). Air pressure is empirically derived using the hypsometric equation and scale height of the atmosphere for midlatitudes.\n",
    "# \n",
    "# The data is first pulled from the NRCS API using metloom. The data is then preprocessed to fill any missing timesteps. MetSim is then used to generate the incoming shortwave radiation. Finally, the remaining meteorological variables are calculated and converted to correct units before saving as a netcdf output file conforming to pysumma naming and formatting conventions.\n",
    "# \n",
    "# ### How to Use\n",
    "# 1. In the cell below, edit desired water year in cell below\n",
    "# 2. Edit SNOTEL station ID, can look up on NRCS National Weather and Climate Center's Interactive Map\n",
    "# 3. Edit outgoing file name - no required format, whatever you choose\n",
    "# 4. Edit outgoing path where you would like the met forcing file for pysumma runs\n",
    "# 6. Run all! \n",
    "\n",
    "# %% [markdown]\n",
    "# **Clinton Alden**\n",
    "# \n",
    "# **Mountain Hydrology Research Group**\n",
    "# \n",
    "# **University of Washington**\n",
    "\n",
    "# %%\n",
    "snotel = str(515) + ':WA:SNTL'\n",
    "water_year = int(2022)\n",
    "out_name = 'harts_WY22' # out_path = input('Enter the output path (ie. ../model/forcings/): ')\n",
    "out_path = './forcings/'\n",
    "\n",
    "print('********** generating the forcing file, please be patient **********')\n",
    "print('********** should take ~3 minutes to run **********')\n",
    "# %% [markdown]\n",
    "# ## Use metloom API to pull snotel data\n",
    "\n",
    "# %%\n",
    "import warnings\n",
    "# pysumma has many depreciated packages, this ignores their warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='scipy')\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from metloom.pointdata import SnotelPointData\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from metsim import MetSim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from metpy.units import units\n",
    "import metpy.calc as mpcalc\n",
    "import math\n",
    "import scipy\n",
    "import os\n",
    "import shutil\n",
    "from pytz import UTC\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/cdalden/summa_setup/model')\n",
    "from utils import lw_clr\n",
    "from utils import forcing_filler as ff\n",
    "from utils import summa_check as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** pulling snotel data **********\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "#  Create needed directories to store metsim run and snotel CSVs\n",
    "if not os.path.exists('./input/'):\n",
    "    os.makedirs('./input/')\n",
    "\n",
    "if not os.path.exists('./out/'):\n",
    "    os.makedirs('./out/')\n",
    "\n",
    "if not os.path.exists('./snotel_csvs/'):\n",
    "    os.makedirs('./snotel_csvs/')\n",
    "\n",
    "if not os.path.exists('./forcings/'):\n",
    "    os.makedirs('./forcings/')\n",
    "\n",
    "# metsim and metloom require different formats of time and ranges, reformat here\n",
    "water_year_str = str(water_year)\n",
    "start_year = water_year - 1\n",
    "start_year_str = str(start_year)\n",
    "\n",
    "start_date = datetime(start_year, 7, 3)\n",
    "end_date = datetime(water_year, 9, 30)\n",
    "\n",
    "spinstart = pd.to_datetime(start_year_str + '-07-03').tz_localize('UTC')\n",
    "spinend = pd.to_datetime(start_year_str + '-09-30').tz_localize('UTC')\n",
    "\n",
    "start_loc = datetime(start_year, 10, 1).replace(tzinfo=UTC)\n",
    "mask_date = datetime(start_year, 10, 2).replace(tzinfo=UTC)\n",
    "\n",
    "dates = pd.date_range('10/01/' + start_year_str, '09/30/' + water_year_str)\n",
    "\n",
    "spin_range = pd.date_range('07/03/' + start_year_str, '09/30/' + start_year_str)\n",
    "print('********** pulling snotel data **********')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Pull desired variables from snotel to dataframe\n",
    "snotel_point = SnotelPointData(snotel, \"MyStation\")\n",
    "df = snotel_point.get_hourly_data(\n",
    "    start_date, end_date,\n",
    "    [snotel_point.ALLOWED_VARIABLES.PRECIPITATIONACCUM, snotel_point.ALLOWED_VARIABLES.TEMP]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from multiprocessing import Process\n",
    "def get_data_for_range(snotel_name, start_d, end_d, pickle_path):\n",
    "    print('Pulling data for: ', snotel_name, ' for range: ', start_d, ' to ', end_d)\n",
    "    start_time = time.time()\n",
    "    df_snotel = SnotelPointData(snotel_name, \"MyStation\")\n",
    "    df = df_snotel.get_hourly_data(\n",
    "        start_d, end_d,\n",
    "        [snotel_point.ALLOWED_VARIABLES.PRECIPITATIONACCUM, snotel_point.ALLOWED_VARIABLES.TEMP]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print('Time to pull data: ', end_time - start_time)\n",
    "    df.to_pickle(pickle_path)\n",
    "    return df\n",
    "\n",
    "MAX_CORES = 20\n",
    "\n",
    "def run_in_parallel(function, params_list):\n",
    "    # params list should be a list of tuples that follow the format of the function:\n",
    "    # for example, for get_data_for_range - (snotel_name, start_date, end_date, pickle_path)\n",
    "    # for example: [('515:WA:SNTL', datetime(2021, 10, 1), datetime(2022, 9, 30), 'snotel_pkls/harts_WY21.pkl'), ...]\n",
    "    print('Running in parallel...')\n",
    "    start_time = time.time()\n",
    "    processes = []\n",
    "    for params in params_list:\n",
    "        p = Process(target=function, args=params)\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    print('Time to run in parallel: ', time.time() - start_time)\n",
    "\n",
    "def make_snotel_data_params():\n",
    "    start_date = datetime(2000, 10, 1)\n",
    "    end_date = datetime(2024, 9, 30)\n",
    "    csv_df = pd.read_csv('/home/cdalden/summa_setup/analysis/sntl_list_ski_temps.csv')\n",
    "    param_list = []\n",
    "    for i in csv_df.iterrows():\n",
    "        site_code = i[1]['site_code']\n",
    "        site_name = i[1]['site_name']\n",
    "        state = i[1]['state']\n",
    "        pickle_path = './snotel_pkls/' + site_name + '.pkl'\n",
    "        param_list.append((str(site_code) + \":\" + state + \":SNTL\", start_date, end_date, pickle_path))\n",
    "    return param_list\n",
    "\n",
    "params = make_snotel_data_params()\n",
    "run_in_parallel(get_data_for_range, params[0:MAX_CORES])\n",
    "run_in_parallel(get_data_for_range, params[MAX_CORES:(MAX_CORES + MAX_CORES)])\n",
    "run_in_parallel(get_data_for_range, params[(MAX_CORES + MAX_CORES):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "MultiIndex: 10846 entries, (Timestamp('2021-07-03 08:00:00+0000', tz='UTC'), '515:WA:SNTL') to (Timestamp('2022-09-30 08:00:00+0000', tz='UTC'), '515:WA:SNTL')\n",
      "Data columns (total 10 columns):\n",
      " #   Column                           Non-Null Count  Dtype   \n",
      "---  ------                           --------------  -----   \n",
      " 0   geometry                         10846 non-null  geometry\n",
      " 1   ACCUMULATED PRECIPITATION        10797 non-null  float64 \n",
      " 2   ACCUMULATED PRECIPITATION_units  10797 non-null  object  \n",
      " 3   AIR TEMP                         10845 non-null  float64 \n",
      " 4   AIR TEMP_units                   10845 non-null  object  \n",
      " 5   SWE                              10843 non-null  float64 \n",
      " 6   SWE_units                        10843 non-null  object  \n",
      " 7   SNOWDEPTH                        10354 non-null  float64 \n",
      " 8   SNOWDEPTH_units                  10354 non-null  object  \n",
      " 9   datasource                       10846 non-null  object  \n",
      "dtypes: float64(4), geometry(1), object(5)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify latitude, longitude, and elevation from station metadata\n",
    "lat = snotel_point.metadata.y\n",
    "lon = snotel_point.metadata.x\n",
    "elev = snotel_point.metadata.z\n",
    "\n",
    "# Clean up the dataframe\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "replace = {'ACCUMULATED PRECIPITATION':'accppt','AIR TEMP':'airtemp', 'datetime':'time'}\n",
    "df.rename(columns=replace, inplace=True)\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "# df.to_csv('./snotel_csvs/'+out_name+'.csv')\n",
    "\n",
    "try:\n",
    "    df.drop(columns=['site', 'ACCUMULATED PRECIPITATION_units', 'geometry', 'AIR TEMP_units', 'datasource', \n",
    "                 'SWE', 'SWE_units', 'SNOWDEPTH', 'SNOWDEPTH_units'], inplace=True)\n",
    "except:\n",
    "    df.drop(columns=['site', 'ACCUMULATED PRECIPITATION_units', 'geometry', 'AIR TEMP_units', 'datasource', \n",
    "                 'SWE', 'SWE_units'], inplace=True)\n",
    "    print('SNOTEL csv has no snowdepth for this run')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Fill missing timesteps from snotel data\n",
    "\n",
    "# %%\n",
    "# Convert the index of the dataframe to a DatetimeIndex\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Create a date range from the first to the last timestep\n",
    "date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Find the missing dates\n",
    "missing_dates = date_range[~date_range.isin(df.index)]\n",
    "\n",
    "# Print the missing dates\n",
    "# print(missing_dates)\n",
    "\n",
    "# Reindex the data DataFrame with the missing dates\n",
    "# Concatenate the original DataFrame with a DataFrame containing the missing dates\n",
    "new_df = pd.concat([df, pd.DataFrame(index=missing_dates)], axis=0)\n",
    "\n",
    "# Sort the new DataFrame by the index\n",
    "new_df = new_df.sort_index()\n",
    "df = new_df\n",
    "\n",
    "# Fill NaNs for every other column\n",
    "df = df.fillna(np.nan)\n",
    "\n",
    "# Rename index\n",
    "df.index.name = 'time'\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Unit Conversions\n",
    "\n",
    "# %%\n",
    "# Covert air temperature to celsius\n",
    "df['airtemp'] = (df['airtemp'] - 32) * 5.0/9.0\n",
    "\n",
    "# Convert precipitation to mm\n",
    "df['accppt'] = df['accppt'] * 25.4\n",
    "\n",
    "# Convert from geodataframe to dataframe\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Split up data into spinup state and desired date range for MetSim\n",
    "\n",
    "# %%\n",
    "# Interpolate the missing values\n",
    "df.interpolate(inplace=True)\n",
    "\n",
    "# Seperate the data into two dataframes, before and after October 1\n",
    "# spinstart = pd.to_datetime('2014-07-03').tz_localize('UTC')\n",
    "# spinend = pd.to_datetime('2014-09-30').tz_localize('UTC')\n",
    "spinup = df.loc[spinstart:spinend].copy()\n",
    "data = df.loc[start_loc:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copy the dataframe a2 to a2_copy\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Create a mask to identify rows where the index is less than or equal to October 2, 2023\n",
    "mask = data_copy.index <= mask_date\n",
    "\n",
    "# Set the 'precip_accum' column to 0 for rows that satisfy the mask condition\n",
    "data_copy.loc[mask, 'accppt'] = 0\n",
    "\n",
    "# Update the value of a2 to the modified copy\n",
    "data = data_copy\n",
    "\n",
    "# Calculate the difference between the maximum value of 'precip_accum' and the previous value\n",
    "spinup['pptrate'] = spinup['accppt'].cummax().diff()\n",
    "data['pptrate'] = data['accppt'].cummax().diff()\n",
    "\n",
    "# Drop accppt column\n",
    "spinup.drop(columns=['accppt'], inplace=True)\n",
    "data.drop(columns=['accppt'], inplace=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Generate SW from MetSim\n",
    "\n",
    "# %%\n",
    "# Create empty dataset\n",
    "shape = (len(dates), 1, 1, )\n",
    "dims = ('time', 'lat', 'lon', )\n",
    "\n",
    "# We are running only one site, at these coordinates\n",
    "lats = [lat]\n",
    "lons = [lon]\n",
    "elev = elev # meters\n",
    "coords = {'time': dates, 'lat': lats, 'lon': lons}\n",
    "\n",
    "# Create the initial met data input data structure\n",
    "met_data = xr.Dataset(coords=coords)\n",
    "\n",
    "# %%\n",
    "for varname in ['prec', 't_min', 't_max']:\n",
    "    met_data[varname] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                     coords=coords, dims=dims,\n",
    "                                     name=varname)\n",
    "\n",
    "# %%\n",
    "# Resample the data to daily frequency and calculate the maximum and minimum temperatures\n",
    "tmax_vals = data['airtemp'].resample('D').max()\n",
    "tmin_vals = data['airtemp'].resample('D').min()\n",
    "\n",
    "# Calculate the daily precipitation values\n",
    "prec_vals = data['pptrate'].resample('D').sum()\n",
    "\n",
    "# Interpolate the temperature values to fill in any missing days\n",
    "# tmax_vals = tmax_vals.interpolate(method='linear')\n",
    "# tmin_vals = tmin_vals.interpolate(method='linear')\n",
    "\n",
    "met_data['prec'].values[:, 0, 0] = prec_vals\n",
    "\n",
    "# Assign the daily maximum and minimum temperatures to the met_data xarray, converting to Celsius\n",
    "met_data['t_min'].values[:, 0, 0] = tmin_vals\n",
    "met_data['t_max'].values[:, 0, 0] = tmax_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "met_data.to_netcdf('./input/rc_forcing.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# We form the domain in a similar fashion\n",
    "# First, by creating the data structure\n",
    "coords = {'lat': lats, 'lon': lons}\n",
    "domain = xr.Dataset(coords=coords)\n",
    "domain['elev'] = xr.DataArray(data=np.full((1,1,), np.nan),\n",
    "                          coords=coords,\n",
    "                          dims=('lat', 'lon', ))\n",
    "domain['mask'] = xr.DataArray(data=np.full((1,1,), np.nan),\n",
    "                          coords=coords,\n",
    "                          dims=('lat', 'lon', ))\n",
    "\n",
    "# Add the data\n",
    "domain['elev'][0, 0] = elev\n",
    "domain['mask'][0, 0] = 1\n",
    "domain.to_netcdf('./input/rc_domain.nc')\n",
    "\n",
    "# %%\n",
    "# Finally, we create the state file - the dates are 90 days prior to \n",
    "# the MetSim run dates - as usual, create an empty data structure to\n",
    "# read the data into\n",
    "shape = (len(spin_range), 1, 1, )\n",
    "dims = ('time', 'lat', 'lon', )\n",
    "coords = {'time': spin_range, 'lat': lats, 'lon': lons}\n",
    "state = xr.Dataset(coords=coords)\n",
    "for varname in ['prec', 't_min', 't_max']:\n",
    "    state[varname] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                               coords=coords, dims=dims,\n",
    "                               name=varname)\n",
    "    \n",
    "# Resample precip to daily\n",
    "prec_vals = spinup['pptrate'].resample('D').sum()\n",
    "\n",
    "# Resample the data to daily frequency and calculate the maximum and minimum temperatures\n",
    "tmax_vals = spinup['airtemp'].resample('D').max()\n",
    "tmin_vals = spinup['airtemp'].resample('D').min()\n",
    "\n",
    "# Do precip data\n",
    "state['prec'].values[:, 0, 0] = prec_vals\n",
    "\n",
    "# And now temp data and convert to C\n",
    "state['t_min'].values[:, 0, 0] = tmin_vals\n",
    "state['t_max'].values[:, 0, 0] = tmax_vals\n",
    "state.to_netcdf('./input/rc_state.nc')\n",
    "\n",
    "# %%\n",
    "# dates = pd.date_range('10/01/2014', '09/30/2015')\n",
    "params = {\n",
    "    'time_step'    : \"60\",       \n",
    "    'start'        : dates[0],\n",
    "    'stop'         : dates[-1],\n",
    "    'forcing'      : './input/rc_forcing.nc',     \n",
    "    'domain'       : './input/rc_domain.nc',\n",
    "    'state'        : './input/rc_state.nc',\n",
    "    'forcing_fmt'  : 'netcdf',\n",
    "    'out_dir'      : './out',\n",
    "    'out_prefix': out_name,\n",
    "    'scheduler'    : 'threading',\n",
    "    'chunks'       : \n",
    "        {'lat': 1, 'lon': 1},\n",
    "    'forcing_vars' : \n",
    "        {'prec' : 'prec', 't_max': 't_max', 't_min': 't_min'},\n",
    "    'state_vars'   : \n",
    "        {'prec' : 'prec', 't_max': 't_max', 't_min': 't_min'},\n",
    "    'domain_vars'  : \n",
    "        {'elev': 'elev', 'lat': 'lat', 'lon': 'lon', 'mask': 'mask'}\n",
    "    }               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run MetSim\n",
    "ms = MetSim(params)\n",
    "ms.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ms.open_output().load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete MetSim input and output directories to declutter, they are unnecessary\n",
    "if os.path.exists('./input/'):\n",
    "    shutil.rmtree('./input/')\n",
    "\n",
    "if os.path.exists('./out/'):\n",
    "    shutil.rmtree('./out/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Create SUMMA forcing netCDF\n",
    "\n",
    "# %%\n",
    "out_df = output.to_dataframe()\n",
    "out_df.reset_index(inplace=True)\n",
    "out_df.set_index('time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Remove timezone from index\n",
    "data.index = data.index.tz_convert(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert precipitation rate from m hr^-1 to kg m^-2 s^-1\n",
    "data['pptrate'] = data['pptrate']/3600\n",
    "\n",
    "# Generate relative humidity assuming T_d is overnight low temperature\n",
    "# Used to calculate specific humidity and longwave radiation\n",
    "ff.fill_rel_hum(data)\n",
    "\n",
    "# Convert airtemp to Kelvin\n",
    "data['airtemp'] = (1.03*(data['airtemp']-0.9)) + 273.15 # Currier snotel temp correction\n",
    "\n",
    "# Simulated warming\n",
    "data['airtemp'] = data['airtemp'] + 1 #K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate pressure from hypsometric equation and site elevation (1981m)\n",
    "ff.fill_pressure(data, elev)\n",
    "\n",
    "# Generate specific humidity\n",
    "ff.fill_spec_hum(data)\n",
    "data['spechum'] = data['spechum'].clip(lower=0.001)\n",
    "\n",
    "\n",
    "# Set shortwave radiation to MetSim output\n",
    "data['SWRadAtm'] = out_df['shortwave']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate longwave radiation\n",
    "data['LWRadAtm'] = lw_clr.dilleyobrien1998(data['airtemp'], data['rh'])\n",
    "\n",
    "# Can alternatively use the MetSim LW radiation\n",
    "# data['LWRadAtm'] = out_df['longwave']\n",
    "\n",
    "# Set wind to 2 m/s\n",
    "data['windspd'] = 2\n",
    "\n",
    "# Fill in missing values\n",
    "data['pptrate'] = data['pptrate'].fillna(0)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['rh'])\n",
    "\n",
    "# Interpolate the missing values\n",
    "data.interpolate(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Load template forcing file to preserve attributes\n",
    "template = xr.open_dataset('/home/cdalden/summa_setup/model/summa_forcing_template.nc')\n",
    "\n",
    "# Convert dataframe to xarray\n",
    "dsx = data.to_xarray()\n",
    "\n",
    "# Loop through variables and add attributes from template forcing file\n",
    "for data_var in dsx:\n",
    "    dsx[data_var].attrs = template[data_var].attrs\n",
    "    \n",
    "# Add hru dimension\n",
    "dsx = dsx.expand_dims(dim={'hru':1})\n",
    "\n",
    "# Add gap-filled and datastep variables\n",
    "dsx['gap_filled'] = xr.DataArray(np.ones((1,dsx.time.shape[0])),dims = ['hru','time'])\n",
    "dsx['data_step'] = 3600 # 3600 seconds for 1hr timesteps\n",
    "\n",
    "# Convert all to float64\n",
    "for var in dsx.data_vars:\n",
    "    dsx[var] = dsx[var].astype(np.float64)\n",
    "\n",
    "# Set hruID based on template\n",
    "dsx['hruId'] = (xr.DataArray(np.ones((1))*template['hruId'].values,dims = ['hru'])).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transpose all variables to match SUMMA dimensions\n",
    "count = 0\n",
    "for var in dsx.data_vars:\n",
    "    # print(var,count)\n",
    "    count += 1\n",
    "    if count <= 7:\n",
    "        attribs = dsx[var].attrs\n",
    "        arr_t = dsx[var].values.T\n",
    "        dsx[var] = xr.DataArray(dims = ['time','hru'],data = arr_t)\n",
    "        dsx[var].attrs = attribs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set hruID based on template\n",
    "dsx['hruId'] = (xr.DataArray(np.ones((1))*template['hruId'].values,dims = ['hru'])).astype(np.float64).fillna(0).astype(np.int32)\n",
    "\n",
    "# Set time to match SUMMA format and save\n",
    "dsx.to_netcdf(out_path+out_name+'.nc',\n",
    "                        encoding = {\"time\":\n",
    "                                        {'dtype' : 'float64',\n",
    "                                         'units' : 'hours since 1990-01-01 00:00:00',\n",
    "                                         'calendar' : 'standard'}})\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** generating the forcing file, please be patient **********\n",
      "********** should take ~3 minutes to run **********\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # SNOTEL Processing for pySUMMA Modeling\n",
    "# \n",
    "# This notebook pulls SNOTEL data via metloom to create a meteorological forcing file for pysumma. 7 input meteorological variables are needed at hourly (if using SNOTEL data, hourly is highest temporal resolution possible) timesteps: air temperature, precipitation, incoming shortwave radiation, incoming longwave radiation, air pressure, relative humidity, and wind speed. \n",
    "# \n",
    "# Temperature used is observed from SNOTEL with the *Currier et al. (2017)* voltage issue correction. Precipitation used is observed from SNOTEL - be wary of undercatch for upper elevation SNOTEL sites in windier locations, check to ensure accumulated precip > max SWE. Incoming shortwave radiation is empirically derived using latitude, elevation, and time of year to calculate clear sky radiation and the diurnal temperature range and precipitation to calculate the cloud correction factor with the MetSim package. Incoming longwave radiation is empirically derived from air temperature and relative humidity using the *Dilley and O'Brien (1998)* method. The empirical calculation method for incoming longwave radiation can be modified if desired - the `lw_clr.py` script in `summa_work/utils` provides a number of different methods to choose from. Relative humidity is empirically derived assuming the running 24 hour minimum temperature as the dewpoint for each timestep from *Running et al. (2017)*. Wind speed is set at 2 meters per second for every timestep as this is an incredibly difficult quantity to observe in mountainous regions during winter due to riming and other issues (*TODO - citation needed for this choice*). Air pressure is empirically derived using the hypsometric equation and scale height of the atmosphere for midlatitudes.\n",
    "# \n",
    "# The data is first pulled from the NRCS API using metloom. The data is then preprocessed to fill any missing timesteps. MetSim is then used to generate the incoming shortwave radiation. Finally, the remaining meteorological variables are calculated and converted to correct units before saving as a netcdf output file conforming to pysumma naming and formatting conventions.\n",
    "# \n",
    "# ### How to Use\n",
    "# 1. In the cell below, edit desired water year in cell below\n",
    "# 2. Edit SNOTEL station ID, can look up on NRCS National Weather and Climate Center's Interactive Map\n",
    "# 3. Edit outgoing file name - no required format, whatever you choose\n",
    "# 4. Edit outgoing path where you would like the met forcing file for pysumma runs\n",
    "# 6. Run all! \n",
    "\n",
    "# %% [markdown]\n",
    "# **Clinton Alden**\n",
    "# \n",
    "# **Mountain Hydrology Research Group**\n",
    "# \n",
    "# **University of Washington**\n",
    "\n",
    "# %%\n",
    "# snotel = input('Enter the desired SNOTEL site code (ie. 1107:WA): ') + ':SNTL'\n",
    "# water_year = int(input('Enter the water year: '))\n",
    "# out_name = input('Enter the output file name (ie. buck_WY16): ')\n",
    "out_name = 'quartzpeak_+1K_WY20'\n",
    "# out_path = input('Enter the output path (ie. ../model/forcings/): ')\n",
    "out_path = '/home/cdalden/summa_setup/model/forcings/'\n",
    "\n",
    "print('********** generating the forcing file, please be patient **********')\n",
    "print('********** should take ~3 minutes to run **********')\n",
    "# %% [markdown]\n",
    "# ## Use metloom API to pull snotel data\n",
    "\n",
    "# %%\n",
    "import warnings\n",
    "# pysumma has many depreciated packages, this ignores their warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='scipy')\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from metloom.pointdata import SnotelPointData\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from metsim import MetSim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from metpy.units import units\n",
    "import metpy.calc as mpcalc\n",
    "import math\n",
    "import scipy\n",
    "import os\n",
    "import shutil\n",
    "from pytz import UTC\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/cdalden/summa_setup/model')\n",
    "from utils import lw_clr\n",
    "from utils import forcing_filler as ff\n",
    "from utils import summa_check as sc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airtemp</th>\n",
       "      <th>pptrate</th>\n",
       "      <th>airpres</th>\n",
       "      <th>spechum</th>\n",
       "      <th>SWRadAtm</th>\n",
       "      <th>LWRadAtm</th>\n",
       "      <th>windspd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-01 00:00:00</th>\n",
       "      <td>273.120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86742.720117</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>233.980875</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 01:00:00</th>\n",
       "      <td>272.193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86695.475664</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>230.704974</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 02:00:00</th>\n",
       "      <td>271.884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86679.660484</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>229.615087</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 03:00:00</th>\n",
       "      <td>271.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86674.444313</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>229.303258</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 04:00:00</th>\n",
       "      <td>272.090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86690.078945</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>230.237365</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 04:00:00</th>\n",
       "      <td>288.982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87559.926306</td>\n",
       "      <td>0.012311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.209573</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 05:00:00</th>\n",
       "      <td>289.291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87578.303669</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>324.294569</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 06:00:00</th>\n",
       "      <td>289.497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87587.720890</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>18.053410</td>\n",
       "      <td>325.080726</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 07:00:00</th>\n",
       "      <td>289.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87592.419474</td>\n",
       "      <td>0.012870</td>\n",
       "      <td>88.472374</td>\n",
       "      <td>325.472164</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 08:00:00</th>\n",
       "      <td>289.394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87583.015607</td>\n",
       "      <td>0.012838</td>\n",
       "      <td>154.988113</td>\n",
       "      <td>324.688189</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8769 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     airtemp  pptrate       airpres   spechum    SWRadAtm  \\\n",
       "time                                                                        \n",
       "2019-10-01 00:00:00  273.120      0.0  86742.720117  0.004171    0.000000   \n",
       "2019-10-01 01:00:00  272.193      0.0  86695.475664  0.004027    0.000000   \n",
       "2019-10-01 02:00:00  271.884      0.0  86679.660484  0.003979    0.000000   \n",
       "2019-10-01 03:00:00  271.781      0.0  86674.444313  0.003970    0.000000   \n",
       "2019-10-01 04:00:00  272.090      0.0  86690.078945  0.003997    0.000000   \n",
       "...                      ...      ...           ...       ...         ...   \n",
       "2020-09-30 04:00:00  288.982      0.0  87559.926306  0.012311    0.000000   \n",
       "2020-09-30 05:00:00  289.291      0.0  87578.303669  0.012822    0.000000   \n",
       "2020-09-30 06:00:00  289.497      0.0  87587.720890  0.012854   18.053410   \n",
       "2020-09-30 07:00:00  289.600      0.0  87592.419474  0.012870   88.472374   \n",
       "2020-09-30 08:00:00  289.394      0.0  87583.015607  0.012838  154.988113   \n",
       "\n",
       "                       LWRadAtm  windspd  \n",
       "time                                      \n",
       "2019-10-01 00:00:00  233.980875      2.0  \n",
       "2019-10-01 01:00:00  230.704974      2.0  \n",
       "2019-10-01 02:00:00  229.615087      2.0  \n",
       "2019-10-01 03:00:00  229.303258      2.0  \n",
       "2019-10-01 04:00:00  230.237365      2.0  \n",
       "...                         ...      ...  \n",
       "2020-09-30 04:00:00  321.209573      2.0  \n",
       "2020-09-30 05:00:00  324.294569      2.0  \n",
       "2020-09-30 06:00:00  325.080726      2.0  \n",
       "2020-09-30 07:00:00  325.472164      2.0  \n",
       "2020-09-30 08:00:00  324.688189      2.0  \n",
       "\n",
       "[8769 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in current climate run for the site\n",
    "current_climate_forcing_file = out_name.replace('+1K', 'current')\n",
    "current_climate_forcing = xr.open_dataset(f'/home/cdalden/summa_setup/model/forcings/{current_climate_forcing_file}.nc')\n",
    "\n",
    "data = current_climate_forcing.to_dataframe()\n",
    "\n",
    "# Generate relative humidity assuming T_d is overnight low temperature\n",
    "# Used to calculate specific humidity and longwave radiation\n",
    "ff.fill_rel_hum(data)\n",
    "\n",
    "\n",
    "# Simulated warming\n",
    "data['airtemp'] = data['airtemp'] + 1 #K\n",
    "\n",
    "# Generate pressure from hypsometric equation and site elevation (1981m)\n",
    "ff.fill_pressure(data, 2000)\n",
    "\n",
    "# Generate specific humidity\n",
    "ff.fill_spec_hum(data)\n",
    "data['spechum'] = data['spechum'].clip(lower=0.001)\n",
    "\n",
    "# Generate longwave radiation\n",
    "data['LWRadAtm'] = lw_clr.dilleyobrien1998(data['airtemp'], data['rh'])\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['rh', 'hru', 'gap_filled', 'data_step', 'hruId'])\n",
    "\n",
    "# Interpolate the missing values\n",
    "data.interpolate(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load template forcing file to preserve attributes\n",
    "template = xr.open_dataset('./model/summa_forcing_template.nc')\n",
    "\n",
    "# Convert dataframe to xarray\n",
    "dsx = data.to_xarray()\n",
    "\n",
    "# Loop through variables and add attributes from template forcing file\n",
    "for data_var in dsx:\n",
    "    dsx[data_var].attrs = template[data_var].attrs\n",
    "    \n",
    "# Add hru dimension\n",
    "dsx = dsx.expand_dims(dim={'hru':1})\n",
    "\n",
    "# Add gap-filled and datastep variables\n",
    "dsx['gap_filled'] = xr.DataArray(np.ones((1,dsx.time.shape[0])),dims = ['hru','time'])\n",
    "dsx['data_step'] = 3600 # 3600 seconds for 1hr timesteps\n",
    "\n",
    "# Convert all to float64\n",
    "for var in dsx.data_vars:\n",
    "    dsx[var] = dsx[var].astype(np.float64)\n",
    "\n",
    "# Set hruID based on template\n",
    "dsx['hruId'] = (xr.DataArray(np.ones((1))*template['hruId'].values,dims = ['hru'])).astype(np.int32)\n",
    "\n",
    "# Transpose all variables to match SUMMA dimensions\n",
    "count = 0\n",
    "for var in dsx.data_vars:\n",
    "    # print(var,count)\n",
    "    count += 1\n",
    "    if count <= 7:\n",
    "        attribs = dsx[var].attrs\n",
    "        arr_t = dsx[var].values.T\n",
    "        dsx[var] = xr.DataArray(dims = ['time','hru'],data = arr_t)\n",
    "        dsx[var].attrs = attribs\n",
    "\n",
    "# Set hruID based on template\n",
    "dsx['hruId'] = (xr.DataArray(np.ones((1))*template['hruId'].values,dims = ['hru'])).astype(np.float64).fillna(0).astype(np.int32)\n",
    "\n",
    "# Set time to match SUMMA format and save\n",
    "dsx.to_netcdf(out_path+out_name+'.nc',\n",
    "                        encoding = {\"time\":\n",
    "                                        {'dtype' : 'float64',\n",
    "                                         'units' : 'hours since 1990-01-01 00:00:00',\n",
    "                                         'calendar' : 'standard'}})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
